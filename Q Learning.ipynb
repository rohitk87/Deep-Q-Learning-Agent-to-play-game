{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q Learning.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMeadGDNlP6f98Fh+oP+w3/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"JTafMK2HTOpE","executionInfo":{"status":"ok","timestamp":1625719677487,"user_tz":-330,"elapsed":449,"user":{"displayName":"skewed exploiter","photoUrl":"","userId":"15273537745458082558"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vKzRDGRTUHO"},"source":["# Exploration Policy"]},{"cell_type":"code","metadata":{"id":"NiYC2qEPTlkg","executionInfo":{"status":"ok","timestamp":1625718612875,"user_tz":-330,"elapsed":531,"user":{"displayName":"skewed exploiter","photoUrl":"","userId":"15273537745458082558"}}},"source":["class ExplorationPolicy:\n","    def __init__(self, epsilon=1.0, epsilon_min=0.01, epsilon_dec=0.00001):\n","        self.epsilon = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_dec = epsilon_dec\n","\n","    def explore(self):\n","        should_explore = random.random() < self.epsilon\n","        self.epsilon = max(self.epsilon-self.epsilon_dec, self.epsilon_min)\n","        return should_explore"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L019QAq9VilT"},"source":["# Replay Buffer"]},{"cell_type":"code","metadata":{"id":"zBRtLgrKaDGD"},"source":["class ReplayBuffer:\n","\n","    def __init__(self, max_size, batch_size, input_shape):\n","        self.memory = collections.deque(maxlen=max_size)\n","        self.batch_size = batch_size\n","        self.input_shape = input_shape\n","    \n","    def save(self, state, action, reward, state_, done):\n","        self.memory.append((state, action, reward, state_, done))\n","    \n","    def sample(self):\n","        sample_size = min(self.batch_size, len(self.memory))\n","        sample = random.sample(self.memory, sample_size)\n","        state, action, reward, state_, done = zip(*sample)\n","        state = np.array(state, dtype=np.float32)\n","        action = np.array(action, dtype=np.int32)\n","        reward = np.array(reward, dtype=np.float32)\n","        state_ = np.array(state_, dtype=np.float32)\n","        done = np.array(done, dtype=np.bool)\n","        return state, action, reward, state_, done"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XgHIZ-DUaII"},"source":["# Agent"]},{"cell_type":"code","metadata":{"id":"9N8sDhC4dwl4"},"source":["class Agent:\n","\n","    def __init__(\n","        self, \n","        gamma=0.99 , lr=0.001,\n","        input_shape, n_actions, \n","        epsilon, epsilon_min, epsilon_dec,\n","        mem_size, batch_size,\n","        model=DQN, buffer=ReplayBuffer, exploration_policy=ExplorationPolicy,\n","        replace_rate,\n","        ):\n","        self.gamma = gamma\n","        self.lr = lr\n","        \n","        self.n_actions = n_actions\n","        self.input_shape = input_shape\n","        self.action_space = [i for i in range(n_actions)]\n","        \n","        self.replace_rate = replace_rate\n","        self.lern_step_cnt = 0\n","        \n","        self.exploration_policy = exploration_policy(epsilon, epslion_min, epsilon_dec)\n","        self.buffer = buffer(max_size, batch_size, input_shape)\n","        \n","        self.q_eval = model(input_shape, n_actions, lr, name = \"_eval\")\n","        self.q_next = model(input_shape, n_actions, lr, name = \"_next\")\n","\n","    def act(self.state):\n","        if self.exploration_policy.explore():\n","            action = np.random.choice(self.action_space)\n","        else :\n","            state = torch.tensor([state], dtype=torch.float32).to_device(self.q_eval.device)\n","            action = self.q_eval(state).argmax(dim=1).item()\n","        return action\n","\n","    def save(self, state, action, reward, state_, done):\n","        self.buffer.save(state, action, reward, state_, done)\n","    \n","    def sample(self):\n","        state, action, reward, state_, done = self.buffer.sample()\n","        \n","        state = torch.tensor(state, dtype=torch.float32)\n","        action = torch.tensor(action, dtype=torch.int32)\n","        reward = torch.tensor(reward, dtype=torch.float32)\n","        state_ = torch.tensor(state_, dtype=torch.float32)\n","        done = torch.tensor(done, dtype=np.bool)\n","\n","        return state, action, reward, state_, done\n","\n","    def replace_target_network(self):\n","        self.learn_step_cnt += 1\n","        if self.learn_step_cnt == self.replace_rate:\n","            self.learn_step_cnt = 0\n","            self.q_next.load_state_dict(q_eval.state_dict())\n","\n","    def learn(self):\n","        self.q_eval.optimizer.zero_grad()\n","\n","        self.replace_target_network()\n","\n","        states, actions, rewards, states_, dones = self.sample()\n","\n","        q_pred = self.q_eval.forward(states)[:, actions]\n","        q_next = self.q_next.forward(states_).max(dim=1)[0]\n","\n","        q_next[dones] = 0.0 # for not updating rewards when game is done\n","        q_target = rewards + self.gamma*q_next\n","\n","        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n","        \n","        loss.backward()\n","\n","        self.q_eval.optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfOcXsrG09u3"},"source":["class DQN(nn.Module):\n","    \n","    def __init__(self, inputs, outputs, lr):\n","        super(DQN, self).__init__()\n","        \n","        self.fc1 = nn.Linear(inputs, 128)\n","        self.fc2 = nn.Linear(128, 256)\n","        self.fc3 = nn.Linear(256, 256)\n","        self.fc4 = nn.Linear(256, outputs)\n","\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n","        self.loss = nn.MSELoss()\n","\n","        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","    \n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = F.relu(self.fc2(out))\n","        out = F.relu(self.fc3(out))\n","        out = self.fc4(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Db24_Kiq62Ys"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"GclKrd_D6zdg"},"source":[""],"execution_count":null,"outputs":[]}]}